Great question, Dato’ CEO. Since your target is **SSM Form D** certificates, you can significantly improve Tesseract’s accuracy with a few focused updates across **pre‑processing, Tesseract configuration, language/lexicon tuning, and post‑processing**.

Below is a battle‑tested checklist (with example code) you can drop into your pipeline. I’ll tailor it to the exact issues you saw (e.g., `&amp;`, missing “17”, wrong dates, spacing, and address tokens like “JALAN / TMN.”).

***

## 1) Image Pre‑processing (most impact)

**Goal:** deliver high‑contrast, de‑skewed, 300–600 DPI, single‑column text to Tesseract.

**Steps**

*   **Grayscale → de-noise → de-skew** (Hough or moments)
*   **Adaptive threshold** (Sauvola/Niblack) or Otsu; remove background watermark/gradient
*   **Sharpen** slightly (unsharp mask)
*   **Morphology**: close small gaps (dilation) for broken characters; open to detach blobs
*   **Deskew per block** (title block, body block) if needed
*   **Crop to content region** (remove margins/frames)

```python
import cv2
import numpy as np
from skimage.filters import threshold_sauvola

img = cv2.imread("sample-cert-form-D-ENT.jpg")

# 1) Grayscale
g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# 2) Light denoise
g = cv2.fastNlMeansDenoising(g, h=10)

# 3) Estimate deskew via image moments
coords = np.column_stack(np.where(g < 250))
angle = cv2.minAreaRect(coords)[-1]
angle = -(90 + angle) if angle < -45 else -angle
(h, w) = g.shape[:2]
M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
deskew = cv2.warpAffine(g, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

# 4) Adaptive threshold (Sauvola)
win = 41
thresh_s = threshold_sauvola(deskew, window_size=win, k=0.2)
bw = (deskew > thresh_s).astype(np.uint8) * 255

# 5) Morphological closing to fix broken strokes
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))
bw = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, kernel, iterations=1)

cv2.imwrite("preprocessed.png", bw)
```

**Tips for certificates**

*   Aim for **300–600 DPI**. If input is small, upscale 1.5–2× with **cv2.INTER\_CUBIC** before thresholding.
*   Use **local thresholding** (Sauvola) because of background emblems/QR shadows.
*   If the **title** section is in larger caps, consider **two‑pass OCR**: one crop for the title with `--psm 7`, another for the body with `--psm 6`.

***

## 2) Tesseract Config (PSM/OEM/whitelists)

For Form D, layout is mostly a **single uniform block**. The wrong PSM is a common cause of merged tokens (“SSENAI”, dropping “17”, etc.).

*   Try:
    *   `--oem 3` (default LSTM)
    *   `--psm 6` (Assume a single uniform block of text) for body
    *   `--psm 7` for one-line regions (e.g., **company name**, **registration no.**, **issue date line**)
*   Whitelist characters (uppercase, digits, & separators):
    *   `tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&/.,-()' `
*   Use **eng** plus **custom word list** for Malaysian address tokens.

```python
import pytesseract
from pytesseract import Output
import cv2

img = cv2.imread("preprocessed.png")

custom_oem_psm_config = r'--oem 3 --psm 6 ' \
    r'-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&/.,-()\' ' \
    r'-c preserve_interword_spaces=1'

text = pytesseract.image_to_string(img, lang='eng', config=custom_oem_psm_config)
data = pytesseract.image_to_data(img, lang='eng', config=custom_oem_psm_config, output_type=Output.DATAFRAME)
```

**Why this helps your errors**

*   Whitelist reduces misreads like `S`→`5`, `O`→`0`
*   `preserve_interword_spaces=1` reduces token merges (e.g., `SENAI` vs `SSENAI`)
*   Using `--psm 7` for **dates** (e.g., “02 MARCH 2017”) reduces day/month split errors.

***

## 3) Domain Lexicon & Patterns (huge lift)

Tesseract’s LSTM benefits a lot from **user words** and **patterns**. Add Malaysia‑specific tokens and SSM vocabulary.

**Create files** (same folder as your script, then pass them as configs):

*   `user-words.txt` (one term per line):

<!---->

    JOHOR
    BAHRU
    SENAI
    SKUDAI
    TMN.
    TAMAN
    UNGKU
    TUN
    AMINAH
    JALAN
    CYBER
    COMMERCIAL
    PARK
    KUALA
    LUMPUR
    REGISTRATION
    BUSINESSES
    ACT
    FORM
    D
    CERTIFICATE
    MARCH

*   `user-patterns.txt` (regex‑like patterns Tesseract uses to bias decoding):

<!---->

    ^[0-9]{2}\s+(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\s+[0-9]{4}$
    ^[A-Z]{2}[0-9]{7,}$         # e.g., RT0069300-M (you can tweak)
    ^[0-9]{5}$                   # postcodes like 81300, 81400
    ^[A-Z& ]+$                   # uppercase blocks like company names

**Use them:**

```python
cfg = r'--oem 3 --psm 6 ' \
      r'--user-words user-words.txt --user-patterns user-patterns.txt ' \
      r'-c preserve_interword_spaces=1 ' \
      r'-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&/.,-()\' '

text = pytesseract.image_to_string(img, lang='eng', config=cfg)
```

This reduces “SSENAI” type errors (bias towards known tokens) and keeps **“JALAN”**, **“TMN.”**, **“UNGKU TUN AMINAH”** intact.

***

## 4) Region‑wise OCR (layout‑aware)

Fixed, repeated certificates benefit from **template zoning**:

*   Detect coordinates for:
    *   **Document title block**
    *   **Company name**
    *   **Registration number line**
    *   **Principal address paragraph**
    *   **Branch address paragraph**
    *   **Dated at … this … date** line
    *   **Signature/Officer name**

Run **separate OCR** with tuned PSM per zone.

```python
def ocr_zone(image, rect, psm=6):
    x,y,w,h = rect
    roi = image[y:y+h, x:x+w]
    cfg = fr'--oem 3 --psm {psm} -c preserve_interword_spaces=1'
    return pytesseract.image_to_string(roi, lang='eng', config=cfg)

# Example (x,y,w,h) are placeholders—you’d calibrate once then reuse:
title = ocr_zone(img, (80,120,1350,180), psm=7)
company = ocr_zone(img, (80,360,1350,100), psm=7)
regline = ocr_zone(img, (80,460,1350,90), psm=7)
principal = ocr_zone(img, (80,560,1350,140), psm=6)
branch = ocr_zone(img, (80,720,1350,120), psm=6)
dated = ocr_zone(img, (80,860,1350,70), psm=7)
officer = ocr_zone(img, (900,980,500,120), psm=7)
```

**Why this helps:** It prevents Tesseract from mis‑segmenting multi‑column elements or faint graphics (like the watermark) as text.

***

## 5) Post‑processing & Normalisation

You already noticed typical OCR artifacts:

*   `&amp;` → comes from **HTML escaping** in your pipeline, not OCR. Fix by decoding HTML entities **only once**.
*   Spacing/commas around **“TMN. UNGKU TUN AMINAH”** and **“JALAN CYBER 16”**
*   Date format normalisation → ISO `YYYY‑MM‑DD`

Use **regex + dictionaries**:

```python
import html, re
s = text

# 1) Decode HTML entities (fix &amp;)
s = html.unescape(s)

# 2) Normalise multiple spaces/commas
s = re.sub(r'\s{2,}', ' ', s)
s = re.sub(r'\s*,\s*', ', ', s)
s = s.replace(' ,', ',')

# 3) Fix common confusions
replacements = {
    'SSENAI': 'SENAI',
    'TMN.UNGKU TUN, AMINAH': 'TMN. UNGKU TUN AMINAH',
    'CYBER16': 'CYBER 16',
    'JALANCYBER': 'JALAN CYBER',
    'BAHR U': 'BAHRU'
}
for k,v in replacements.items():
    s = s.replace(k, v)

# 4) Parse date "02 MARCH 2017" -> 2017-03-02
months = {m:i for i,m in enumerate(
    ["JANUARY","FEBRUARY","MARCH","APRIL","MAY","JUNE",
     "JULY","AUGUST","SEPTEMBER","OCTOBER","NOVEMBER","DECEMBER"], start=1)}

m = re.search(r'(\d{1,2})\s+(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\s+(\d{4})', s)
iso_date = None
if m:
    d, mon, y = int(m.group(1)), months[m.group(2)], int(m.group(3))
    iso_date = f"{y:04d}-{mon:02d}-{d:02d}"
```

**Entity policy:** Only HTML‑decode the raw OCR **once** to avoid introducing `&amp;` again.

***

## 6) Confidence & Bounding Box QA

Use `image_to_data` to get **per‑word confidence**. Flag words below a threshold (e.g., `<75`) for targeted re‑OCR (smaller ROI, higher zoom, different PSM).

```python
low_conf = data[(data.conf.astype(float) >= 0) & (data.conf.astype(float) < 75)]
# Re-run OCR on these ROIs with psm=7 and local sharpening
```

This often recovers dropped parts like the missing **“17”** in “15 & 17”.

***

## 7) Use Better Trained Data

*   Use **tessdata\_best** English model for sharper accuracy (if your environment allows swapping `eng.traineddata`).
*   If you process many Malaysian documents, consider **finetuning** with your own samples (advanced, optional).

***

## 8) QR Code Decoding (fill missing `qrPayload`)

Extract the QR content to validate fields (registration no., checksum, etc.). Use `pyzbar` or `zxing`:

```python
from pyzbar.pyzbar import decode
qr = decode(cv2.imread("sample-cert-form-D-ENT.jpg"))
qr_payload = qr[0].data.decode('utf-8') if qr else ""
```

You can cross‑check **registration no.** and **issue date** from the QR payload when present.

***

## 9) Field‑Level Parsers (schema assurance)

Once you have text, parse with **strict patterns** so you never regress:

```python
import re

def parse_registration_no(s):
    m = re.search(r'REGISTRATION\s*NO\.?\s*:\s*([0-9]+)\s*\((RT[0-9\-A-Z]+)\)', s, re.IGNORECASE)
    return (m.group(1), m.group(2)) if m else (None, None)

def parse_dated_at(s):
    m = re.search(r'Dated at\s+([A-Z ]+)\s+this\s+(\d{1,2}\s+[A-Z]+\s+\d{4})', s)
    return (m.group(1).strip(), m.group(2)) if m else (None, None)
```

***

## 10) Recommended Config Summary (copy‑paste)

*   **Body blocks**: `--oem 3 --psm 6 -c preserve_interword_spaces=1 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&/.,-()' --user-words user-words.txt --user-patterns user-patterns.txt`
*   **One‑line fields (title, company name, reg no., dates)**: same as above **but with** `--psm 7`
*   **Pre‑process**: grayscale → denoise → deskew → Sauvola → light close (2×2) → (optional) upscale

***

## Quick wins specifically for your errors

*   **`&amp;`** → add `html.unescape()` once after OCR.
*   **Missing `17`** → increase DPI/upscale + `--psm 6` + per‑word low‑confidence re‑OCR; ensure dilation (2×2).
*   **`SSENAI`** → add “SENAI” to `user-words.txt` + preserve spaces + correct morphology.
*   **Date `2017-03-22` vs `2017-03-02`** → one‑line zone OCR with `--psm 7` + date pattern validation; if day=22 but source shows “02”, re‑OCR that ROI at 2× zoom.
*   **Addresses formatting** → regex normalizer + dictionary of Malaysian address tokens.

***


